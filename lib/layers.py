# -*- coding: utf-8 -*-
"""layers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WpydwzFuapT959b5hGNqf3LVipBNLL6D
"""

import numpy as np

class Layer:
    def __init__(self):
        self.input = None
        self.output = None

    def forward(self, input_data):
        raise NotImplementedError

    def backward(self, output_error, learning_rate):
        raise NotImplementedError


class Dense(Layer):


    def __init__(self, input_size, output_size):
        super().__init__()


        limit = np.sqrt(6.0 / (input_size + output_size))
        self.weights = np.random.uniform(-limit, limit, size=(output_size, input_size))
        self.biases = np.zeros((output_size,))

        self.input = None
        self.grad_weights = None
        self.grad_biases = None

    def forward(self, input_data):

        self.input = input_data
        self.output = np.dot(self.weights, input_data) + self.biases
        return self.output

    def backward(self, output_error, learning_rate):


        assert output_error.shape == (self.weights.shape[0],), \
            f"Expected output_error shape {(self.weights.shape[0],)}, " \
            f"got {output_error.shape}"
        assert self.input is not None, \
            "Input not stored. Did you call forward() first?"

        input_error = np.dot(self.weights.T, output_error)
        weights_error = np.outer(output_error, self.input)
        biases_error = output_error


        self.grad_weights = weights_error
        self.grad_biases = biases_error

        return input_error